[00:00:00] hello all my name is krishak and welcome to my YouTube channel so guys I'm super excited to bring this one short video on
[00:00:07] understanding how to build ETL pipelines using Apachi airflow airflow is one of the most open- Source platform that are
[00:00:13] being used by many many many companies uh and it is being used by both Big Data engineering team and data science team
[00:00:20] and our main target over here is to probably understand how ETL pipeline is developed that is extract transform load
[00:00:27] and how we can automate this entire workflow with the help of airflow right so before we proceed this particular tutorial I also want to
[00:00:33] quickly make one amazing announcement my new course on complete mlops boot camp with 10 plus end to end ml projects is
[00:00:40] live and here you will probably be learning so many different different mlops tools I've covered almost every
[00:00:45] mlops tools and as we go ahead I will also be uploading about CU flow and all which is one of the most requested topic
[00:00:51] by many many people right now this course is in just 399 and I will keep on regularly updating you with the coupon
[00:00:58] codes of this specific emic course where you'll be able to get the course in 399 so let's go ahead enjoy this particular
[00:01:05] session and make sure that you try out this particular course also so yes let's go towards the tutorial thank you hello
[00:01:11] guys so let's go through the agenda and I will be talking about what all things we are going to cover in this specific
[00:01:17] video right so first of all here I have written down all the topics that we are
[00:01:23] going to learn so let me just go ahead and write down the agenda okay so the agenda over here is that we will first
[00:01:29] of all understand what is Astro and uh if you all don't know what exactly is
[00:01:35] Astro we'll still Deep dive more into this particular platform but this platform completely manages the entire
[00:01:42] airflow uh through which you will be able to schedule the workf flows okay along with this we'll also try to
[00:01:48] understand what is airflow and then we will do a amazing practical implementation of creating an ETL
[00:01:55] pipeline now this entire ETL pipeline what exactly it is we'll also be discussing about it you know and finally
[00:02:01] we'll deploy this ETL pipeline using Astro and AWS so this is the agenda of
[00:02:06] this entire one-hot video and uh we'll try to understand each and everything step by step now before we go ahead and
[00:02:13] talk more about this particular topic let's go ahead and talk about a simple life cycle of a data science project
[00:02:19] okay now whenever we talk about the life cycle of a data science project initially Whenever there is a data
[00:02:25] project in this particular step you know in this step one so the first step that
[00:02:30] whenever we get a data science project the first thing that the domain expert or product owner and the business
[00:02:36] analyst does is that they write down all the requirements with respect to what all modules we really need to develop in
[00:02:42] this particular data science project right and based on the discussion between the two team what they will do
[00:02:48] is that they'll JW down all the requirements if they following an agile process they'll make sure that in every
[00:02:54] Sprints right so they will probably divide all the stories based on multiple Sprints so let's say in the Sprint one
[00:03:00] they want to cover two to three stories in Sprint two they want to probably go ahead and cover four to five modules
[00:03:07] Sprint three they want to probably cover four to five another modules right so this way they will jaw down all the
[00:03:13] requirements and they will be making sure that they will divide all the stories based on this particular Sprint
[00:03:18] and that is how the hiring of a data science team basically happens now it is
[00:03:23] not like whenever I say about a data science team you know I'm just only considering data analyst or data
[00:03:29] scientist they may be domain expert they may be product owner they may be business analyst and many more people okay now after the requirement Gathering
[00:03:36] is done which is my second step we go to the third step that is this all requirements are basically sent to the
[00:03:42] data analyst or data scientist team now data analyst along with the data scientist team along with the domain expert and product owner they will first
[00:03:49] of all understand that in order to solve this particular data science project where all they need really need to
[00:03:55] consider or what all data they really need to consider in order to solve the problem statement right so my uh so for
[00:04:01] this particular problem statement the data may be present in the internal database right or it may be in some
[00:04:07] third party or cloud apis or iot devices right they may be multiple sources right
[00:04:13] if it is present in the internal database then this domain expert or product owner will be knowing about that
[00:04:19] and they will be able to provide suggestion for the data analyst or data scientist team but this two team will be
[00:04:25] identifying the right kind of data so let me write it down over here this two
[00:04:30] team will be identifying okay identifying the right kind of data in
[00:04:36] order to solve this problem okay identifying the right kind of data once
[00:04:41] they determine like which all sources they have to really consider taking the data then it goes to the Big
[00:04:49] Data engineering team where the big data engineering team will create data
[00:04:54] pipelines okay data pipelines now this data pipeline one example of the data pipeline that that I would like to
[00:05:00] consider is something called as ETL pipeline right ETL
[00:05:05] pipeline Now to create this kind of ETL pipeline because this ETL pipeline needs to be run daily it can be weekly it can
[00:05:12] be monthly because continuously we will be getting the data from all this particular sources now it is really
[00:05:17] important that whenever we talk about ETL pipeline let me just go ahead and Define it over here what
[00:05:23] exactly is a ETL pipeline so ETL pipeline is derived from
[00:05:30] mainly three different words one is extract the second one is something
[00:05:36] called as transform and the third one is something called as
[00:05:41] load okay so this three main components actually creates your ETL pipeline now
[00:05:49] whenever we talk about extract right now you have seen from that data pipeline
[00:05:54] picture that I've actually shown you right the data sources may be multiple
[00:05:59] there may be Source One there may be Source two there may be Source three right let's consider this is one kind of
[00:06:06] DB this can be from this data may be coming from some iot right this Source
[00:06:12] three data may be coming from some paid apis right now when we have so many
[00:06:18] different sources right as the data engineering team as a data engineering team what
[00:06:27] they really need to do is that they need to combine combine all the sources of data right they need to combine all the
[00:06:34] sources of data and once they combine all the sources of data right what they really need to do let's say that I will
[00:06:40] be performing some kind of transformation over here because if I probably consider after the big data
[00:06:46] engineering team you know after this your life cycle of a data science project continues right the life cycle
[00:06:53] of data science project continues and whenever I talk about data science I'm
[00:06:59] talking about about feature engineering I'm talking about feature selection I'm talking about model training model
[00:07:05] evaluation all these things we'll start over here right so here what will happen is that in this retail pipeline the data
[00:07:12] engineering team will combine all the sources of this particular data and let's say that the entire data is
[00:07:18] basically converted into a Json right with that many number of features let's say this source of data is coming up
[00:07:24] with four features this is coming up with five features this is coming up with another seven features we we may
[00:07:29] combine all these particular features and we may finally convert this into a Json which is a transformation step and
[00:07:35] finally we take it to the next step wherein we load this entire data into
[00:07:41] some other source right now here my source can be mongodb right it can be postra SQL right
[00:07:52] and again it depends on what kind of databases or it can also be an S3 bucket we can also use an S3 bucket in AWS now
[00:07:59] again depends on the kind of project requirement it is right but this exactly gives you an idea like how an ETL
[00:08:06] pipeline basically Works extract transform and load we are extracting
[00:08:12] from multiple sources we are extracting the data from multiple sources in the transform stage we are basically doing
[00:08:18] some kind of transformation where we are combining all the data over here and finally we may be converting into a Json
[00:08:24] or we may be converting into a data frame we may also convert this into a data frame and finally we may store this
[00:08:32] in our some kind of data source and that is the reason we say it as load here we are saying it as transformation here we
[00:08:38] are seeing it as extract okay now this is good okay but always remember that
[00:08:44] this sources of data will also keep on getting updated right this keeps on getting
[00:08:51] updated with respect to time keeps on getting updated with
[00:08:58] time yes or no yes obviously it needs to get updated with time like today I may
[00:09:04] have some other data tomorrow the iot devices May create more data right so it is important that we run this ETL
[00:09:11] pipeline regularly let's say we we need to run this entire ETL pipeline regularly it is
[00:09:18] not a one-time job right probably we need to run it daily there may be some scenarios that
[00:09:24] we need to run it weekly right so here in order to
[00:09:30] probably run this this will definitely be running in the form of workflows right and that is where we come up with
[00:09:36] workflows in workflows what happens is that we know that one step after the
[00:09:42] other right in ETL pipeline we have to run in this format we have to first of all extract the data do the
[00:09:48] transformation and finally load it and so finally we need to probably go ahead and create a workflow right but this
[00:09:54] workflow needs to be scheduled regularly right and how we can schedule this
[00:10:00] workflow for this we will be using something called as airflow now this airflow is an open-source platform from
[00:10:08] Apachi right open source platform and whatever things we have discussed over
[00:10:14] here with respect to schedule with respect to running it right and running this ETL pipeline
[00:10:21] not only ETL pipeline you can even run any kind of pipeline right your data science pipeline where you're doing
[00:10:28] feature engineering model training model evaluation everything you will be able to run it with the help of airflow and
[00:10:34] that is why airf flow is super super important for implementing the entire
[00:10:40] ETL pipelines or any kind of data pipelines that we are working in right now most of the Big Data engineering
[00:10:46] team in many many companies airf flow is specifically used okay now the second
[00:10:54] question that we are trying to answer right what exactly is airf flow I hope you have got an idea about it but now
[00:11:00] comes what is Astro Astro is a platform which is also called as
[00:11:06] astronomer doio and in short if I really want to talk about this platform it basically
[00:11:13] manages the entire airflow manages the airf flow because if
[00:11:19] I really want to run airf flow in the form of Dockers and all astronomer basically plays a very good role in that
[00:11:25] okay so now what I'm actually going to do with the help of AST we will try and with the help of Astro and astronomer we
[00:11:32] will develop an ETL pipeline implementation wherein let's say that we will be having an API we will read the
[00:11:38] data from the API and we will probably do some kind of transformation and we'll be loading in some kind of database and
[00:11:45] all these things we will try to deploy it using Astro and AWS and we will be using some Cloud which is called as
[00:11:51] Astro Cloud platform okay so step by step I will be showing you each and everything as we go ahead now first of
[00:11:58] all what I will do this is what astronoma doio looks like okay and uh if
[00:12:05] you just see over if I click on airflow right Apache air flow the DOR standard for expressing data flows as code Apache
[00:12:12] air flow is the open source standard used by data professional to autor schedule and manage workflows so this
[00:12:17] entire astronoma manages the entire airflow itself in a much more efficient
[00:12:23] way okay so we will be specifically using this and here uh you'll be able to find some amazing documentation with
[00:12:29] respect to this okay so if you probably see this these are some of the reports that are there with respect to how airf
[00:12:36] flow is doing and all uh it is right now in amazing growth because many many companies are specifically using okay
[00:12:43] now what I can also do is that I can also go ahead and search for Astro dashboard sorry documentation
[00:12:50] okay I'll write I'll just search for Astro airflow documentation so this is what is my
[00:12:57] Astro airflow documentation if I go ahead and click on on this here it talks about how you can go ahead and create a
[00:13:02] deployment you can automate the entire cicd right each and everything and many many companies are getting specifically
[00:13:08] using this if I click on get started with Astro CLI here it is you know first of all we need to probably go ahead and
[00:13:14] do the installation I will show you how you can probably go ahead and do the installation I can click on this install
[00:13:20] the CLI and all the rules and uh steps are basically written over we'll go step by step and I'll show you how to
[00:13:25] implement this entire ETL pipeline okay now along with this we'll also be using a post gr SQL and that I will show you
[00:13:32] over here like uh I will try to deploy this in my AWS and show you step by step
[00:13:38] right everything will be provided to you from code to anything that you specifically required okay and this is
[00:13:44] your Apache air flow that we have okay now let's go ahead and develop our pipeline so here I will create one
[00:13:50] folder let's say I I'm just canceling this let's create a folder this folder
[00:13:57] name I will just create as ETF uh the API that I am want to going to
[00:14:02] use is something called as weather okay so uh okay let's see where ETL weather
[00:14:08] is basically created right so inside this I will go over here inside this particular folder okay from this folder
[00:14:14] I will just go ahead and open my command prompt okay now from the command prompt what I will do I will just go ahead and
[00:14:20] write code dot where I will start with my VSS code okay so my VSS code is
[00:14:26] opened okay so this is how my VSS code looks like I usually use vs code for doing all my
[00:14:32] project implementation because I like it it's good enough it's very good and all right now what I'm actually going to do
[00:14:39] from this is that I will just go ahead and open my terminal okay so this is how my terminal looks like I will just go
[00:14:45] ahead and open my command Pro now the first step is that how do we go ahead and install astronom okay so I will just
[00:14:54] go ahead and minimize this so if I go ahead and see over here you can just search for astronoma documentation
[00:15:00] anyhow I'll be giving the link in the description of this particular video so here you'll be able to see install the
[00:15:05] CLI first of all we need to go ahead and install this Astro CLI and this is a very amazing platform guys with the help
[00:15:12] of this particular platform from folder structure to developing the entire airflow application it becomes very
[00:15:18] simple okay now here what I'll Be Seeing is that there are multiple options for Mac you just need to go ahead and
[00:15:24] probably write this particular command uh in your terminal and you should be able to install for Windows uh there are some steps
[00:15:31] first of all you need to have Dockers so I have my Docker ready over here so here you can see all my containers is there
[00:15:37] and all my containers we specific uh like I keep on playing with this all you can see Astro is there ETL pipeline is
[00:15:44] there grafana is there I I keep on exploring multiple things as we go ahead you know so what I'm actually going to
[00:15:50] do over here is that see you need to have Docker if you're having Docker desktop that basically means you need to
[00:15:55] have this Microsoft hyperv enabled it should be done and here Windows minimum
[00:16:00] this much is required in order to probably work with airf flow okay now the first command is basically to just
[00:16:06] go ahead and install it by using this okay so here open the window shell as administrator and then run the following
[00:16:12] command okay so you need to run this command in order to install Astro right so that is required in Windows manual
[00:16:18] also you have all the steps you have to just go ahead and run this particular command okay and similarly if you want
[00:16:24] to do it in Linux you have this curl command which you can probably go ahead and install it
[00:16:29] so what I will do I will just go ahead and take this command and then I will open my vs
[00:16:38] code okay in the terminal you just need to go ahead and copy and paste this once
[00:16:44] you press enter the installation of the Astro will take place okay uh I've
[00:16:49] already done the installation so I'm not going to go ahead and execute it again because anyhow it'll just show me requirement already satisfied okay so
[00:16:56] this is the command we basically use it and I've already told you how you can go ahead and uh come up with that
[00:17:02] particular command okay now to start a airflow project what you really need to
[00:17:07] do is that if you have installed Astro right you just need to go ahead and write Astro Dev in it okay and once you
[00:17:14] do the installation please make sure that you restart your vs code okay once you restart your vs code then you'll be
[00:17:20] able to see the code itself okay so here I'm going to go ahead and write Astro Dev in it so here you see it is being
[00:17:26] initializing an astro project pulling a flow development files from Astro runtime 12.1.1 and initialized an empty
[00:17:33] Astro project in this particular folder location okay so here you'll be seeing that automatically my entire project
[00:17:41] folders has been created here I have my Docker file here I have my requirement. txt right I have my airflow settings.
[00:17:48] EML file here is my dags which we will be specifically creating with the help of airflow so and here is one example
[00:17:54] with respect to the airflow itself right now let me do one one thing let me just go ahead and talk more theory about air
[00:18:01] flow okay so in airflow your air flow right so here
[00:18:07] whenever we need to create our workflows so if you're using airflow to
[00:18:15] create our workflows okay to create
[00:18:21] workflows okay then we use something called as Dag
[00:18:29] what does dag mean directed directed a cyclic
[00:18:38] graph now what does directed a cyclic graph basically mean okay so here every
[00:18:45] workflow every step in the workflow that we basically Define we have to consider
[00:18:50] this as one task let's say this is my task one right for this particular project pulling the
[00:18:57] data from API I have to pull the data from API so this
[00:19:03] becomes my task one okay this becomes my task one then after pulling the data what we
[00:19:10] need to do the task two we need to perform something called as transformation right so here we will be
[00:19:19] performing something called as transformation okay this is my second
[00:19:26] step that basically means this is my task TK two okay then I have my third important task
[00:19:35] because after I probably do the transformation in the next step I need
[00:19:40] to take the data and let's say in this particular use case I will try to push this into
[00:19:47] some database like post Grace okay I'll push the data to the post Grace So this
[00:19:52] becomes my task three now when we say directed a cyclic graph what does this
[00:19:58] basically mean means directed okay so there some order okay from this node to this node if you want to go first of all
[00:20:05] we need to execute this then only we'll be able to execute this right then once we execute this then only we'll be able
[00:20:10] to execute this so this will specifically go in sequence okay yeah you can also go ahead and execute it
[00:20:16] parallel if you want but in our scenario one after the other when we say directed
[00:20:21] cyc graphs there is some direction right how the execution basically happens when we say a cyclic we cannot have
[00:20:29] this kind of Loops okay once this is executed again we trying to execute this
[00:20:34] or from here we should not have this entire a uh in a cyclic way we should not have so it is basically called as a
[00:20:41] cyclic graph okay and here the graph concept is only there so this is basically my nodes we basically say this
[00:20:48] as nodes and this we have something called as graphs or Direction okay so I
[00:20:54] not say graph but at least a direction nodes uh uh this is my node and this is what is my line over here with respect
[00:21:01] to this particular node which connects one task to the other task okay so this is what entirely my airflow looks like
[00:21:09] right now with respect to implementation also we will follow the same strategy okay what we will do is that we will go
[00:21:15] ahead and create our dag and while creating the dag you know we'll be
[00:21:20] making sure that we create task Now by default there is one example that is
[00:21:25] already given over here okay so here what we are doing is that we are just
[00:21:30] reading some apis over here we are getting the data and we are just displaying the data it's okay don't
[00:21:37] worry if you have not understood this because I will create a new file over here so and I have to create a new file
[00:21:42] inside my Dax folder itself so let me just go ahead and write over here ETL uh ETL and I just say
[00:21:50] weather weather dopy okay ETL weather. py now
[00:21:58] when we are developing this okay ETL weather. py so first of all I will go
[00:22:05] ahead and import from airflow I'm going to import a library which is called as dog okay oops which is called as dag
[00:22:13] okay and then along with this we have two different operation we
[00:22:19] need to read from an API now whenever we need something to read from the API then
[00:22:25] what airflow provides is that they provide some kind of hooks okay okay let's say if I'm probably reading it
[00:22:30] from the API that is basically an HTTP request so for an HTTP request there is a hook which is called as HTTP hook okay
[00:22:38] similarly in airflow they are different different hooks so I will go ahead and write from airflow. providers do HTTP do hooks do
[00:22:49] HTTP import HT sorry HTTP hook okay so this is the
[00:22:58] library that we will be specifically using okay then we know that after we
[00:23:03] read from the API we need to perform some kind of transformation and finally we need to push the entire data into the
[00:23:10] postgress SQL so similarly like if I want to push into postgress SQL there is
[00:23:15] a postgress hook for it also which is provided by airflow not only that if you're going to push your data into S3
[00:23:21] there will be a separate hook if you want to push your data into my SQL there will be a separate hook so you can
[00:23:26] definitely go ahead and follow all the document and you you can see that hey for my any Source DB do I have any kind
[00:23:33] of hooks right so that also you'll be able to find it out okay so now here what I'm going to do I'll just go ahead
[00:23:38] and write from airflow. providers okay providers.
[00:23:44] postgress do hooks do postrace
[00:23:50] import postrace hook and this is the hook that we are
[00:23:56] going to specifically use for pushing our data inside our postra SQL then
[00:24:02] along with that I will also go ahead and import from airflow. decorators I'm going to import something
[00:24:09] called as task and with the help of this particular task we will go ahead and create the task inside our dag okay then
[00:24:16] finally there will also be something like from airflow. utils
[00:24:21] dodes I'll use this doils do dates import days underscore ago okay
[00:24:32] perfect now this we have actually done it okay now what is the kind of task
[00:24:37] that I'm actually going to do let's consider that we have some longitude and latitude okay based on the longitude and
[00:24:43] latitude we need to probably take out the we'll use a weather app API which is
[00:24:49] a free API which will be available for everyone and with the help of that weather app API as soon as we provide
[00:24:55] latitude and longitude we should be able to get some in information some climatic information about that particular place
[00:25:01] okay so I will first of all go ahead and use some latitude and longitude and this
[00:25:07] time the latitude and longitude for a desired location I'm going to take London so this is basically the latitude
[00:25:13] and longitude of London so I'm defined two constants over here now once I have defined the
[00:25:20] latitude and longitude what I am actually going to do is that I will also go ahead and give my connection ID okay
[00:25:27] so let's go ahead and give my post Grace connection ID so here I'm going to basically go ahead and write post Grace
[00:25:33] post Grace because this will basically uh see I'm going to run my post SQL initially into a Docker container right
[00:25:40] and for that I have also have to create an image separately and I have to create a Docker compose okay so post
[00:25:47] Grace post gracecore connection ID okay so I'm going to specifically use this postgis
[00:25:53] connection ID and I can provide any pois connection ID uh as per my my convenience okay so here I will go ahead
[00:26:00] and write it down so here let me just go ahead and write post
[00:26:05] gracecore default so this will basically be my connection ID and uh I will talk
[00:26:11] about it like how you can go ahead and create this connection inside the airflow itself okay and then I will be
[00:26:16] also creating my API uncore connection _ ID which will be nothing but
[00:26:22] opencore meore API okay so this will basically be my just my connection name
[00:26:29] okay for uh for our sake so that we keep a note of it okay now let us go to the
[00:26:35] next step so here I will go ahead and create some default arguments so here let me go ahead and write default uncore
[00:26:41] args is equal to first of all I will just say I'll give some information like
[00:26:48] owner and I'll give the info like airflow so this is a basic setup that we basically require in our airflow okay
[00:26:55] and then here I will also go ahead and say start date we need to provide provide this from which date we are basically going to start over here so
[00:27:01] here I will go ahead and use days uncore ago I'll just give one okay you'll see like what will be our start date once we
[00:27:08] assign this okay you'll be able to understand it now this is some of the default arguments we go ahead with now
[00:27:13] the first step is basically to create our directed a cyclic graph dag now in
[00:27:18] order to create a dag we will be using withd dag okay with dag here we going to use
[00:27:25] our dag ID the dag ID ID that we going to specify over here is nothing but uh
[00:27:32] that basically means my entire dag name so if you see in this example also there is something we have we have probably
[00:27:38] given some kind of dag name right so here not specified any name over here so by default it may take this particular
[00:27:45] example as a tag over there right but we'll see that but here what we'll do is that uh let's see whether dagor ID was
[00:27:51] given no it was not given so here we'll try to give this dag name over here and we have to give it inside dags uncore ID
[00:27:58] okay so this will basically be my weather etore pipeline okay so this is
[00:28:04] my first information let's go to the second information and uh the kind of API that I'm actually using it is called
[00:28:10] as opencore Meto I will talk about that also as we go ahead okay now along with
[00:28:16] that I will be having this default arguments which we have already assigned okay so we'll assign this default
[00:28:22] arguments then we will also go ahead and assign our schedule _ interval Okay so
[00:28:28] schedule uncore interval I will just go ahead and use at theate Daily okay I'll say daily you need to run this
[00:28:34] automatically okay uh so this will basically be my thing uh over here with respect to running and there there is
[00:28:41] also a parameter which is called as catchup so we'll try to talk about it as we go ahead once we execute the code so
[00:28:47] here uh we have to probably create this and now all the code I will be writing it over here now inside this dag we will
[00:28:54] go ahead and Define our task so this is how we go and start a dag okay here we
[00:28:59] have the dag name we G the default arguments what how we need to run this
[00:29:05] at what interval we need to run this now we will go ahead and create our task we have imported this task uh already
[00:29:12] library in the in in the top over here you can see that we have already imported it so we will be using this as
[00:29:18] a decorator and this decorator over here like this will and we have to Define our
[00:29:24] task name or task function so I say hey definition extract because first of all we need to go ahead and extract our
[00:29:31] weather data okay so this is basically my weather data now to extract the weer
[00:29:37] data here I'm going to use my open Meto and all right so open Meto I hope I
[00:29:43] don't many of you may not be knowing but this is a open source it it provides you uh the complete free U uh climatic data
[00:29:52] I guess for any places okay so you will be able to extract any weather data from this API and here this is what we also
[00:29:59] going to set up in the airflow connection I'll talk about what exactly is airflow connection and all okay now
[00:30:06] since we are going to interact with one API so for this we definitely require something called as hooks okay so here
[00:30:14] we are going to use HTTP hook to get the
[00:30:20] weather data okay so how to create this particular hook we
[00:30:26] have already imported so I'll go ahead and create a variable HTTP hook and let
[00:30:31] me go ahead and initialize this HTTP hook the first of all things that I need to specify is my API connection ID okay
[00:30:39] so this API connection ID will be assigned in my
[00:30:44] httpcore connection uncore ID okay so this is where we are going to
[00:30:50] specifically use this httpcore connection ID and this is what we are going to initialize it over there the
[00:30:56] connection ID right because this connection ID needs to be created in the airflow connection I will be showing you
[00:31:01] in the later stages how to do that okay then I'm going to use a method which is called as get so let's go ahead and
[00:31:09] probably use the method like get okay it should be in capital letter so this I have actually made the connection of the
[00:31:15] HTTP hook and remember this connection information will be connect uh so this
[00:31:22] HTTP hook will get the connection details from airflow connection okay
[00:31:28] just like a configuration in airflow okay you can just say see in that way so airflow connection okay so once we do
[00:31:36] this the next thing is that we build the API end point okay we build this
[00:31:44] specific API endo and that is what we are going to see uh over here the API Endo will look something like this
[00:31:52] okay so here I'm just going to go ahead and write my endpoint is equal to I'll
[00:31:58] paste it over here so this is how my API endpoint looks like V1 forecast comma
[00:32:04] latitude latitude and longitude I need to find out and I just need to go ahead and set current weather is equal to True
[00:32:10] once I set it I will be getting some kind of information now what is the URL
[00:32:15] uh the front URL uh what exactly it is uh that also you really need to know
[00:32:21] right because without that particular URL you'll not be able to find out like hey where from which which uh API we are
[00:32:27] specific hitting it right so let me just also make sure that write a comment over
[00:32:32] here for the URL so this is how my URL looks like okay and once I add this
[00:32:38] information at the end okay and let's copy this and let me open my browser
[00:32:47] okay let me open my browser over here now once I hit this URL see I put some
[00:32:52] latitude and longitude so let's put some latitude and longitude value okay
[00:32:57] latitude and longitude value now in this scenario let me go over there in my code
[00:33:04] here I'm going to put my Lang latitude as 5150 74 I'll copy and paste it over
[00:33:12] here and my longitude I will try to keep it as minus1
[00:33:18] 1278 okay just to show you like what data we will be getting right and that entire API you should be able to see it
[00:33:25] so I will just copy this I'll paste it over here so once I execute this this is the
[00:33:32] information that I'm able to get see time interval temperature wind speed and
[00:33:38] all right and here are all the other information that you'll be able to see current weather weather code weather
[00:33:44] code this wind direction temperature interval uh time all this information is
[00:33:50] basically over here right 13.7 you know so all this information is basically
[00:33:55] here for that longitude and latitude so this information we are just going to read and perform some kind of transformation so that is what we have
[00:34:01] created over here that is nothing but my end point okay so once the endpoint is basically created then the next step
[00:34:08] will be that I will go ahead and hit that particular Endo so we'll make the
[00:34:13] request request via the HTTP hook okay
[00:34:18] so I will just go ahead and write response is equal to http hook Dot and
[00:34:24] we'll just use run as a function and what we going to do is that we're going to just give our Endo inside this so let
[00:34:32] me go ahead and write our endpoint over here so once we write the end point that basically means we'll be able to get the
[00:34:37] response and then I will just go ahead and write one if else condition so that you know uh just to make sure that if
[00:34:45] the status code is 200 then we just going to return it in the form of Json otherwise we are going to fail to fetch
[00:34:51] the weather data like kind of information I'm putting it okay now this step is very simple now coming to the
[00:34:57] next step is about transforming your weather data okay now transforming your weather data is very simple okay again
[00:35:04] we'll go ahead and create at theate task and there will be a function definition transform underscore weather data I'm
[00:35:09] giving my weather data over here so weather data of current weather see we have to read something like of a current
[00:35:15] weather this information will probably come see this current weather we'll read it and from this current weather we are going to read all the information right
[00:35:23] so here you'll be able to see that I have this particular current weather right so now what we are doing over here is that we are reading the current
[00:35:29] weather we're getting the information with respect to latitude longitude current weather temperature wind speed wind direction weather code and this
[00:35:35] information we'll try to convert uh it in the form of key value Pairs and this entire information will store it in our
[00:35:42] SQL DB okay now for the postgress SQL DB again uh I will just show you the uh
[00:35:49] that will basically be my next task and over here what I will do I will go ahead
[00:35:55] and create my next task okay and next task is nothing but once I have
[00:36:01] this transform data I have to push this into my database okay so again for
[00:36:07] pushing it I'm creating this particular task which is nothing but load weather underscore data which is nothing but transform data here I'm creating a
[00:36:14] postgress hook why because postgress Hook is very important because I need to push my data into the post SQL right and
[00:36:21] here I have my post connection ID I have my postra connection ID over here I'm giving it then I'm getting the get
[00:36:27] connection I'm getting the cursor okay so first of all we need to go ahead and create a table if the table exist or not
[00:36:33] so here we are going to create a table of weather data okay then we have this latitude longitude temperature float
[00:36:39] wind speed wind direction weather code time stamp all this information we really want to store the information in
[00:36:44] our DB so we are creating this particular table with the help of python right then and this will only execute if
[00:36:50] the table does not exist okay so here we have seen this right create table if not exist weather weather data then what we
[00:36:56] have to do we have to take this trans transform data and we have to push it into the weather data right you can see
[00:37:01] all the information we are pushing it over here remember one thing this load weather data required the transform data
[00:37:07] itself so the flow of execution is really important first we need to extract the weather data then after
[00:37:13] extracting we need to give this response to this transform data transform weather data and finally we have to give this
[00:37:19] transform weather uh transform data to the load weather data okay so in this way only we will go ahead and create our
[00:37:25] data workflow so quickly let's go ahead and create our data workflow so here I'm going to go ahead and write D workflow
[00:37:33] and we will execute this in the form of ETL pipeline okay so first of all I will be having my weather data my weather
[00:37:40] data will extract the weather data over here then
[00:37:47] I have my transformed underscore data transformed uncore data and let's go
[00:37:52] ahead and uh execute the transform weather data I have my weather underscore data
[00:37:59] okay then let's load the weather data and here you have your transformed data
[00:38:08] okay done so this way we executing first of all extract weather data then this
[00:38:13] data we are passing it to the transform weather data the transform data will come then finally we are passing it to the load weather data now let's see
[00:38:21] let's run this and let's see whether everything will run or not but before running this right we need to also go
[00:38:26] ahead and create one more file and that file is something called as Docker compost file okay so let me just go
[00:38:33] ahead and write Docker compos do yml file okay now this dock compos file is
[00:38:40] really really important because we are going to see the post G seal will run it somewhere right initially if I really
[00:38:45] want to run it in our local we need to run it as a container now in order to run it as a container this will be my
[00:38:51] Docker compose DOL file and the prerequisite is that you need to know about Dockers so here I'm creating I'm
[00:38:57] calling a post Grace image which will be pulled from the docker Hub of 13 my container name will be postrace _ DB my
[00:39:05] post Grace username will be postrace password will be postra uh and DB will be postrace and by default the port is
[00:39:12] 5432 and I'm also going to create a volume volumes basically means this will
[00:39:17] like if I stop my application my data should not be lost you know even though I probably run the ETL in my local so
[00:39:23] what we'll do is that we'll create a backup over here with respect to this right and in this location my backup
[00:39:29] will be stored okay uh in where lib pogress data pogress SQL data in this
[00:39:35] particular location my data will be keep on storing so that even though if I restart my data should not be lost okay
[00:39:41] so this is what is my entire coding with respect to my project Now quickly let me go ahead and show you how you can run
[00:39:48] this entire code now in order to run this it is very simple um what you really need to do is that just go ahead
[00:39:53] and write Astro Dev start now as as soon as you write this first of all what
[00:39:59] Astro will do is that it will run this entire airflow package in a container and then along with that because my
[00:40:05] Docker compost has another uh container needs to be run for the postgress it will also run that so here I'll just go
[00:40:12] ahead and write AST Dev start now here you can see that my entire uh airf flow is basically getting buil you can see
[00:40:18] that all this things are coming up and this air flow is starting up and if you probably go ahead and see my container
[00:40:24] let me go ahead and minimize this a new container should have created see ETL weather data right and here you have
[00:40:32] your postgress which is basically running right then you have your web server also running and all everything
[00:40:37] right so let's let's this run and automatically once itun runs right you'll be able to see a browser will get
[00:40:43] opened automatically okay on a specific uh location uh that will'll just see uh
[00:40:49] on which location on which Port see automatically this has got loaded and by default the airflow uh password username
[00:40:56] and password is admin admin so I've written admin admin I'll go ahead and sign in okay so here as soon as we open
[00:41:02] this guys you can see that we are getting one error broken dag uh users local airflow dags ETL weather. py okay
[00:41:10] so this this this error is basically coming up okay user local uh airflow
[00:41:16] dags let me just go ahead and see in ETL weather. py I think some of the import
[00:41:21] uh mistake has happened so let me just go ahead and check it out okay and let's
[00:41:26] see whether any import mistakes have happened or not
[00:41:33] so now now let's run this because I also
[00:41:38] did not import request and Json I'll just go ahead and restart this okay we also did not import this two libraries
[00:41:45] because we are using this two libraries over here right Json and all okay let's
[00:41:51] uh again if you really want to restart this uh you just need to go ahead and write the command Astro Dev restart okay
[00:41:56] after making any changes so now again this will get completely restarted and
[00:42:02] now I think it should be able to get started I guess let's see whether we'll
[00:42:07] face any problems if not I think uh it should run
[00:42:13] perfectly now I think your entire libraries has been loaded correctly okay
[00:42:18] let's go over here and still it is running so let's wait till this gets run completely so this needs to get executed
[00:42:26] completely then only you a flow will get started okay and if I talk about the Dockers here you'll be able to see the
[00:42:31] port also 5432 8080 so at this particular URL my my entire uh uh
[00:42:37] airflow application is running okay so this is run let me just go ahead and reload it so perfect now you'll be able
[00:42:44] to see the my weather ETL pipeline because I did not import Json and uh the request Library so this is my example
[00:42:50] astrona by default Astro gives this but we are very much focused over here now this is how your entire dag
[00:42:59] pipeline looks like you know and the best thing about uh airflow is that it provides you this entire UI it provides
[00:43:05] you the graph like what all things we really need to run as in the form of task see here you have extract weather
[00:43:10] data transform weather data load weather data then you have the task duration you
[00:43:16] have Gant you have graph each and everything right so let's go ahead and run this uh in order to run it here you
[00:43:23] can see schedule daily next run at this particular date you know all the information is over here and if you just
[00:43:29] go and see in dags also you'll be able to find out this information next run when daily is basically set up and
[00:43:34] everything so I'll go inside this if I really want to manually trigger it I'll just go ahead and click on run button
[00:43:40] okay now see over here um as soon as I run this here you can see that I'm getting an error okay now what is the
[00:43:47] error the error is saying that extract weather data from open Meto API using airflow connection so this is a problem
[00:43:53] why this particular problem is coming because I did not set up my connection so let's go ahead and set up my connection now in order to set up the
[00:43:59] connection you you have to remember that what all information I had set up in my
[00:44:05] connection ID so two connections I need to set it up okay one is my postgress
[00:44:10] default the post connection so I'll copy this and I'll paste it over here let's go ahead and click on this I'll put up
[00:44:17] the connection ID over here and then I will just go ahead and select on post
[00:44:22] Grace okay then it is asking me for the host now what host I need need to put up
[00:44:28] over here so for putting up the host you need to go to the docker you need to see that inside my postrace what is the name
[00:44:35] of the container you know where my post Grace is running so this is basically my container name where my postrace is
[00:44:41] running how do you go ahead and see it see inside this container you just go ahead and see where the post is running over here just go ahead and double click
[00:44:47] it and this will be your container name okay so I'll copy this and I'll put it in the host okay
[00:44:54] you know my database name was post Grace my login was nothing but post
[00:45:00] Grace and my password was nothing but post Grace how I'm getting this information for my Docker compose right
[00:45:06] since my Docker compose I have written post Grace post Grace post Grace right and the default Port is 5432 so let me
[00:45:12] just go ahead and set up my port 5 432 once I go ahead and save it so this is
[00:45:18] my first connection that I really wanted to create okay now coming to the second connection again I'll go to my code over
[00:45:26] here you have something called as open Meto API right now open Meto API I will
[00:45:32] just go ahead and create my connection this will be my connection ID this will be an HTTP one right so HTTP request
[00:45:39] I'll go ahead and select and like this you have lot of different different options see sqlite you have FTP email
[00:45:44] elastic search Azure so that is what is so amazing about apachi airf flow right it provides you almost each and
[00:45:50] everything then it is asking me for host so host uh I have to use this particular
[00:45:56] API UR l so let me just go ahead and hit this and copy it and paste it over here
[00:46:01] I don't have to give any login because there is no login or no password that is required and uh in extra I don't have to
[00:46:08] save anything now let's go ahead and save this okay so these are the two information that I really require now
[00:46:13] what I will do I will go to my Apachi airflow sorry I'll go to my this one dags okay now let's go ahead and run
[00:46:21] this initially it got failed now let's go ahead and execute it this time I'm
[00:46:26] getting a green color which is good enough perfect see everything is running perfectly so let's go ahead and see in
[00:46:32] with respect to graph okay so extract weather data so first of all we are inside our weather data we'll go ahead
[00:46:38] and see our logs in the logs you can see uh it is hitting this particular data it
[00:46:43] is giving all the information but if you really want to see like what information we are able to extract it just go and see in this XCOM so this is my entire
[00:46:50] current weather data that I've actually got and we are reading this right we are reading this particular information
[00:46:55] right so I have this entire information that we are getting it over here right wind direction anything as such now one
[00:47:02] step is done but what about a transformed weather data in transform weather data if I go and see in the
[00:47:07] escom you'll be able to see this is the information I got transformed right the entire information we transformed in
[00:47:12] this particular way then again similarly if you go and see in the graph uh and if I go ahead and see in my XCOM you'll be
[00:47:18] able to see that uh here I don't have any information because the last step was pushing the data into my database
[00:47:26] right which is p which is running in the Dockers right so here is my entire logs
[00:47:31] and here you'll be able to see that all the information is over here right and uh how to test it whether my data is
[00:47:38] going going into the post gra or not so for that you can go to your containers
[00:47:43] okay and you can see whether your post Grace is running or not if you see over here 5432 your post Grace is running if
[00:47:50] you click over here nothing will open because your postgress database is running inside a Docker right so in in
[00:47:56] order to connect to that particular post G SQL I will be using something called as DV okay so DB is nothing but it is an
[00:48:06] amazing open source tool again which will actually help you to connect with any kind of databases that you want so
[00:48:13] first of all go ahead and download it whether your system is Windows Mac OS or Linux based on this just go ahead and
[00:48:19] download it okay so if you download it if you install this you'll be seeing that your uh your daver looks something
[00:48:26] like this okay okay now what I'll do I'll just go ahead and delete this and let me just show you once more okay so
[00:48:32] here I will go ahead and connect to a database here I'm going to use postrace I'll click on next okay here my host is
[00:48:40] local postrace database and my username and password is also post okay I will go
[00:48:45] ahead and test the connection and then I will try to see whether any database any data has got updated or not when I'm
[00:48:51] testing the connection if I'm getting this information that basically means it has successfully got connected now now I'll go ahead and click on finish okay
[00:48:59] now if you go ahead and see whether my database has been created or not if you remember my table name is something
[00:49:04] called as weather data and this weather data is basically being created over here remaining all tables are related to
[00:49:11] Dag don't worry about it but here inside my weather data I have so many different columns now let me do one thing let me
[00:49:17] just go ahead and open my SQL script and now see you will be able to see it completely live so if I go ahead and
[00:49:22] execute select star from weather data okay and let's go
[00:49:28] ahead and execute this control enter here right now you'll be able to see one record right because we have just run it
[00:49:34] one time let's go ahead and run it once more inside this okay so we'll go and run it once more I know the same record
[00:49:41] will get updated but we'll see I'm trying just trying to see that whether my data is coming or not so here also it
[00:49:47] is showing green if it is showing green that basically means everything has worked successfully the entire pipeline has run successfully now again I'll go
[00:49:53] to my D VI and again I'll go ahead and write select star form whether data now here you can see my second record has
[00:49:58] been updated right and this is my other information because see this is a live data that I was getting updated
[00:50:04] continuously like this I can go ahead and run it for multiple times as many number of times I run as many number of
[00:50:11] times I run see I'll just keep this everything in pipeline I can run this workflow any
[00:50:17] number of times now I probably run it four number of times and you can see that one by one everything is getting
[00:50:22] executed now if I go to my dwer and again go ahead and execute it you'll be able to see see all my record is getting
[00:50:27] updated right isn't it amazing and this is what I'm actually trying to show you everything is in live
[00:50:34] like everything is happening properly okay uh I hope you have got an idea with
[00:50:39] respect to this uh the next thing will be that how do you probably take this into deployment now see here uh in AWS I
[00:50:48] have created an uh postd SQL once I create this post GD SQL right uh you
[00:50:53] need to just take this endo and you need to update this end Point only in the connection so once you probably go ahead
[00:51:00] and update in the connection over here that's it that basically whatever I do
[00:51:05] operation over here it will automatically get saved in my database that is present in the AWS okay so I
[00:51:12] hope you were able to understand this entire video I hope you are able to understand this one shot of airflow
[00:51:18] wherein we are creating an ETL Pipeline with the help of air flow and Astro and how easy it was basically to manage each
[00:51:24] and everything so yes this was it for my side uh I hope you like this particular video again uh if you like this
[00:51:30] particular video